# Support Vector Machine #

## Defination ##

### In Short ###

#### What is SVM ####
A support vector machine constructs a **hyperplane or set of hyperplanes in a high- or infinite-dimensional space**, 
which can be used for classification, regression, or other tasks.

#### Why is SVM ####

Original classification problem may be stated in a finite dimensional space, it often happens that 
the sets to discriminate are not linearly separable in that space. For this reason, it was proposed 
that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably
making the separation easier in that space.

The mappings used by SVM schemes are designed to ensure that dot products may be computed easily in 
terms of the variables in the original space, by defining them in terms of a kernel function $k(x,y)$ 
selected to suit the problem.

#### What is optimal ####

A good separation is achieved by the hyperplane that has the largest distance to the nearest training 
data point of any class (so-called functional margin), since in general the larger the margin the lower 
the generalization error of the classifier.

### Motivation ###

In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of 
p numbers), and we want to know whether we can separate such points with a (p âˆ’ 1)-dimensional 
hyperplane. This is called a linear classifier. 

There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane 
is the one that represents the largest separation, or margin, between the two classes. So we choose 
the hyperplane so that the distance from it to the nearest data point on each side is maximized. If 
such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it 
defines is known as a maximum margin classifier; or equivalently, the perception of optimal stability.

So we are trying to find a plane in space. Any hyperplane can be written as the set of points 
$\mathbf{x}$ satisfying:
$$
\omega^T\mathbf{x} - b = 0
$$
Then we want those points on different sides of the plane with different labels should have largest 
minimum distance (also equal) to the plane. Let us start our journey with a review of some besics of geometry.

## Details ##

### Some Facts of Geometry ###

#### Inner product ####

FIrst is the inner product.
Inner product is one of the most important concept in the Geometry. We know that we define the inner product as:
$$
x \cdot y = x^Ty = \sum_{i=1}^d x_iy_i
$$
This can be also expressed as:
$$
x \cdot y = \|x\|\|y\|\cos\theta = \text{Proj}_{y}(x) \|y\| = \text{Proj}_{x}(y) \|x\|
$$
Here $\theta$ is the angel degree between two vectors, $\text{Proj}_{y}(x)$ means the **projection of x vector on y vector**.
Later on we will see
that the distance from a point to a hyperplane can be illustrated as a inner product between the normal vector $\omega$
and the point itself plus a constant scale $b$. 

#### Distance from point to a plane ####

Then, we have to recall that one plane in Euclidean Space can be expressed as:
$$
\omega^Tx + b = 0, \;\;\;\; x \in R^d
$$
Here $\omega$ is the **normal** of the hyperplane. For any point $x_i$ that is on this hyperplane, 
it should have $\omega^Tx_i + b = 0$.

Then a common question is how to calculate the distance from a point outside of plane to this hyperplane.
Suppose the point is $x_0 \in R^d$, and we also can randomly find another point $x_1 \in R^d$ that is on the 
hyperplane. Then we have a vector $x_0 - x_1$, and the distance between $x_0$ and the plane is nothing but
the norm of the projection of vector $x_0 - x_1$ to the **normal** of the plane ($\omega$). And we know that
the projection is just the inner product of two vectors.
$$
Dist = \|\text{Proj}_{\omega}(x_0 - x_1)\| = \frac{\|\omega^T(x_0 - x_1)\|}{\|\omega\|} = \frac{\|\omega^Tx_0 + b\|}{\|\omega\|}
$$

Here is one of **important insight about SVM** is that:
- If a point is very close to a hyperplane, the projection of the point to the normal vector should be very small
- however, if a point is far away from the hyperplane, then the projection of the point to the normal should be very large.

### Functional and Geometric Margins ###

#### Logistic Regression ####

If we still remember the logistic regression, we know that we are fitting the probability of classifing to the "1" group as:
$$
\pi_i = h_\theta(x_i) = g(\theta^Tx_i) = \frac{1}{1 + e^{-\theta^Tx_i}}
$$
Here the $g$ is the logistic function. It is a function valued from 0 to 1. When $\pi_i > 0.5$ we classify it as group "1",
and when $\pi_i < 0.5$ we classify it as group "0". The cutoff point $\pi_i = 0.5$ is for $\theta^Tx_i = 0$. So what decides
the classification result is the value of $\theta^Tx_i$, which we want $\theta^Tx_i \gg 0$ for $y=1$, and $\theta^Tx_i \ll 0$
for $y=0$.

Let us look at one simple example as following:

![logistic regression](./figures/logistic.png)

The line in the graph is the $\theta^Tx = 0$ which is also the learning result, $\theta$ is the normal vector. What logistic
regression wants is all data points falling far away from the center line. But this is hard, because if we want some points
to be far away from the line, then we have to sacrify other points to be close to the line just like a seesaw. And this is 
where SVM is different or better than logistic regression since SVM is only care about some of the data point which we call
them support vectors. We will come to this later with more details.

#### Functional Margins ####

First of all, SVM classifier can be introduced as:
$$
h_{\omega,b}(x) = g(\omega^Tx + b)
$$
This is very similar to the logistic regression which $g$ is the logistic function. In SVM, we will define a different $g$
function which is just: $g(z) = 1$ if $z \ge 0$ and $g(z) = -1$ if $z < 0$.

\[ g(z) = \left\{
  \begin{array}{l l}
     1 & \quad \text{if} z \geq 0 \\
    -1 & \quad \text{if} z < 0
  \end{array} \right.\]
  
Given a trainning example $(x_i, y_i)$, we define the functional margin of $(\omega, b)$ with respect to the training example:
$$
\hat \gamma_i = y_i(\omega^T x_i + b)
$$
This is very similar to the logistic regression setup, which we just replace the $\theta$ with $\omega$ and change $\theta_0$ 
to be $b$. if $y_i = 1$, then we need $\omega^Tx + b$ to be a large positive number for $y_i = 1$ and to be a large negative 
number for $y_i = -1$. So overall, if $y_i(\omega^Tx_i + b) > 0$, then our prediction on this example is correct. Hence, a
large functional margin represents a confident and correct prediction.

However, there is one property of the functional margin that makes it not a very good measure of confidence. If we replace 
$\omega$ with $2\omega$ and $b$ with $2b$, then $g(\omega^Tx+b) = g(2\omega^Tx+2b)$ will not change our classification
result at all. So the classification result only depends on the sign, but not on the magnitude of $\omega^Tx + b$. Intuitively
it might therefore make sense to impose some sort of normalization condition to the $\omega$ and $b$.

So given a trainning set, we define the functional margin of $(\omega, b)$ with respect to training set to be the smallest of
the functional margins of the individual training examples:
$$
\hat \gamma = \min \hat \gamma_i
$$

#### Geometric Margins ####

The geometric margin is just a scaled version of the functional margin.

![geometric margin](./figures/gmargin.png)

As shown in the graph, suppose we got the classifier hyperplane. Distance from point A to the hyperplane is $\gamma_i$, and
assume point B is the projection of A on the plane, and A is training example $(x_i,y_i)$.
Recall from the distance to a plane session, we have:
$$
\gamma_i = \frac{\|\omega^Tx_i + b\|}{\|\omega\|}
$$
which can be further rewrote as:
$$
\gamma_i = y_i\left( (\frac{\omega}{\|\omega\|})^Tx_i + \frac{b}{\|\omega\|}\right)
$$
Now it is not hard to notice geometric margins is nothing but a rescaled functional margins with $\frac{1}{\\\omega\|}$.
Still we define the geometric margin to be:
$$
\gamma = \min \gamma_i
$$
You can think the functional margin, just as a testing function that will tell you whether a particular 
point is properly classified or not. And the geometric margin is functional margin scaled by $\|\omega\|$ which is exactly the
normalization condition we mentioned above. But why does the geometric margin exists?
Well to maximize the margin you need more that just the sign, you need to have a notion of magnitude,
the functional margin would give you a number but without a reference you can't tell if the point is 
actually far away or close to the decision plane. The geometric margin is telling you not only if 
the point is properly classified or not, but the magnitude of that distance in term of units of 
$\|\omega\|$ 

Also, the geometric margin is invariant to rescaling of parameters, like functional margin. When trying to fit $\omega$ and $b$
to training data, we can impose an arbitrary scaling constraint on $\omega$ without changing anything important; 
for instance, we can demand that $\|w\| = 1$ and this can be satisfied simply by rescaling $\omega$ and $b$. This arbitrary
constraint however does make our solution unique.

#### Optimal Margin Classifier ####

The goal in SVM learning is try to find a hyperplane, which means find the $\omega$ and $b$, that can make the margin from the
closest point to the plane to be as small as possible. So we do not care about all the points excepts those closest points (
sometimes, closest point is not unique). Then the problem can be translated as:
$$
\begin{aligned}
\max_{\gamma, \omega, b} & \gamma \\
s.t. & y_i(\omega^Tx_i + b) \ge \gamma, \;\;\; i = 1, \cdots, m \\
& \|\omega\| = 1
\end{aligned}
$$
Here the constraint $\|\omega\| = 1$ is just to make $y_i(\omega^Tx_i + b)$ to be geometric margin as well. However since
$\|\omega\| = 1$ is not convex, we just rewrite the problem to be:
$$
\begin{aligned}
\max_{\gamma, \omega, b} & \frac{\hat \gamma}{\|\omega\|} \\
s.t. & y_i(\omega^Tx_i + b) \ge \hat \gamma, \;\;\; i = 1, \cdots, m \\
\end{aligned}
$$
Here we are still trying to maximize the geometric margin because $\gamma = \frac{\hat \gamma}{\|\omega\|}$. We get rid of the
$\|\omega\| = 1$ now, but our objective is not convex anymore. We still have to rewrite it again. Recall that we said changing
$\omega$ and $b$ at the same time will not change the functional margin or geometric margin. So we can just define 
$\hat \gamma = 1$ for convenient to get unique solution.
$$
\begin{aligned}
\max_{\gamma, \omega, b} & \frac{1}{\|\omega\|} \\
s.t. & y_i(\omega^Tx_i + b) \ge 1, \;\;\; i = 1, \cdots, m 
\end{aligned}
$$

Moreover, the objective function or just the likelihood function for logistic regression is:
$$
L(\theta) = \sum_{i=1}^m( y_i\log(g(\theta^Tx_i)) + (1 - y_i)\log(1 - g(\theta^Tx_i))
$$

### Primal Optimization ###

First of all, consider a problem of the following form:

$$
\begin{aligned}
\min_{\omega} & f(\omega) \\
s.t. & h_i(\omega) = 0, \;\;\; i = 1, \cdots, l 
\end{aligned}
$$

It is obvious that the method of Lagrange multipliers can be used to solve it. In this method, we
define the **Lagrangian** to be
$$
\mathcal{L}(\omega, \beta) = f(\omega) + \sum_{i=1}^{l}\beta_ih_i(\omega)
$$
Here the $\beta$'s are called the **Lagrangian multipliers**. We would then find and set $\mathcal{L}$'s
partial derivatives to zero:
$$
\frac{\partial\mathcal{L}}{\partial\omega_i} = 0; \;\;\; \frac{\partial\mathcal{L}}{\partial\beta_i} = 0
$$
and solve for $\omega$ and $\beta$, since the second derivative will give us $h_i(\omega) = 0$

But sometimes, the constraint are not just $h_i(\omega) = 0$. We may have inequality and equality constraints.
So the problem will become as following, which we call the **primal** optimization problem:
$$
\begin{aligned}
\min_{\omega} & f(\omega) \\
s.t. & g_i(\omega) \le 0, \;\;\; i = 1, \cdots, k \\
& h_i(\omega) = 0, \;\;\; i = 1, \cdots, l 
\end{aligned}
$$
To solve it, we start by defining the **generalized Lagrangian**
$$
\mathcal{L}(\omega, \beta) = f(\omega) + \sum_{i=1}^{k}\alpha_ig_i(\omega) + \sum_{i=1}^{l}\beta_ih_i(\omega)
$$
Here, $\alpha_i$ and $\beta_i$ are Lagrange multipliers. And for the multipliers associated with inequality,
we should have further constraint: $\alpha_i \ge 0$. Consider the quantity:
$$
\theta_{\mathcal{P}}(\omega) = \max_{\alpha, \beta: \alpha_i \ge 0} \mathcal{L}(\omega, \alpha, \beta)
$$
Here, the "$\mathcal{P}$" subscript stands for "primal". For this quantity, we fix the $\omega$, and vary 
$\alpha$ and $\beta$. If the given $\omega$ violates any primal constraints for some $i$, the we will have 
$$
\theta_{\mathcal{P}}(\omega) = f(\omega) + \max_{\alpha} \sum_{i=1}^{k}\alpha_ig_i(\omega) + \max_{\beta} 
\sum_{i=1}^{l}\beta_ih_i(\omega) =  \infty
$$
It is obvious that we can easily find $\beta_i$'s to make $\sum_{i=1}^{l}\beta_ih_i(\omega)$ to be $\infty$ if
$h_i(\omega) \ne 0$. Same for the $\alpha_i$'s
Conversely, if the constraints are indeed satisfied for the given value of $\omega$, then 
$\theta_{\mathcal{P}}(\omega) = f(\omega)$. Hence,

\[ \theta_{\mathcal{P}}(\omega) = \left\{
  \begin{array}{l l}
    f(\omega) & \quad \text{if $\omega$ satisfies primal constraints}\\
    \infty & \quad \text{otherwise}
  \end{array} \right.\]

Thus, $\theta_{\mathcal{P}}$ takes the same value as the objective in our problem for all value of $\omega$ 
that satisfies the primal constraints, and is positive infinity if constraints are violated. So if we consider
the minimization problem:

$$
\min_{\omega} \theta_{\mathcal{P}}(\omega) = \min_{\omega} \max_{\alpha, \beta: \alpha_i \ge 0} \mathcal{L}(\omega, \alpha, \beta)
$$

will be the same problem (and has the same solutions as) our original primal problem. For later use, we define 
the optimal value of the objective to be 

$$p^* = \min_{\omega} \theta_{\mathcal{P}}(\omega)$$

#### Note ####

One thing should keep in mind is that in the primal problem (primal optimization), we first maximize our objective function 
$\mathcal{L}(\omega, \alpha, \beta)$ with respect to $\alpha$ and $\beta$ for a given $\omega$, because we can tell if this 
given $\omega$ is satisfied constraints or not by checking if the maximization of $\alpha$ and $\beta$ is infinity or not.
And then minimize over $\omega$, this basically just identify $f(\omega)$ from infinity.

### Dual Optimization ###

Now, besides the primal problem, let's look at a slightly different problem. We define that 
$$
\theta_{\mathcal{D}}(\alpha, \beta) = \min_{\omega}\mathcal{L}(\omega, \alpha, \beta)
$$
Here, the "$\mathcal{D}$" subscript stands for "dual". Note that the difference between $\theta_{\mathcal{D}}$ 
and $\theta_{\mathcal{P}}$:
- $\theta_{\mathcal{P}}(\omega) = \max_{\alpha, \beta} \mathcal{L}(\omega, \alpha, \beta)$, 
and then $\min_{\omega} \theta_{\mathcal{P}}(\omega)$
- $\theta_{\mathcal{D}}(\alpha, \beta) = \min_{\omega} \mathcal{L}(\omega, \alpha, \beta)$, 
and then $\max_{\alpha, \beta} \theta_{\mathcal{D}}(\alpha, \beta)$

So the **dual** optimization problem is:

$$
\max_{\alpha, \beta: \alpha_i \ge 0} \theta_{\mathcal{D}}(\alpha, \beta) = \max_{\alpha, \beta: \alpha_i \ge 0} \min_{\omega}
\mathcal{L} (\omega, \alpha, \beta)
$$

This is exactly the same as our primal problem, except that the order of the "max" and "min" are now exchanged.
We also define the optimal value of dual problem's objective to be:
$$
d^* =  \max_{\alpha, \beta: \alpha_i \ge 0} \theta_{\mathcal{D}}(\alpha, \beta)
$$
Since we know the "max min" of a function always being less than or equal to the "min max", we can have
$$
d^* = \max_{\alpha, \beta: \alpha_i \ge 0} \min_{\omega} \mathcal{L} (\omega, \alpha, \beta) \le
\min_{\omega} \max_{\alpha, \beta: \alpha_i \ge 0} \mathcal{L}(\omega, \alpha, \beta) = p^*
$$
However under certain conditions, we will have $d^* = p^*$. So that we can solve the dual problem in lieu of the primal problem.

Assumption:
- $f$ and $g_i$'s are convex
- $h_i$'s are affine, which means there exists $a_i$ and $b_i$ so that $h_i(\omega) = a_i^T\omega + b_i$. Affine means the same
thing as linear, except that we also allow the extra intercept term $b_i$
- constraints $g_i$ are strictly feasible, which means there exists some $\omega$ so that $g_i(\omega) < 0$ for all $i$.

Under these three assumptions, there **must** exist $\omega^*, \alpha^*, \beta^*$ so that $\omega^*$ is the solution to the 
primal problem, $\alpha^*, \beta^*$ are the solution to the dual problem, and moreover, $p^* = d^* = \mathcal{L}(\omega^*, \alpha^*, \beta^*)$.
Moreover, $\omega^*, \alpha^*, \beta^*$ satisfy the **Karush-Kuhn-Tucker (KKT) conditions**, which are as follows:
$$
\begin{aligned}
\frac{\partial}{\partial \omega_i}\mathcal{L}(\omega^*, \alpha^*, \beta^*) = 0 \;\;\; i = 1, \cdots, d \\
\frac{\partial}{\partial \beta_i}\mathcal{L}(\omega^*, \alpha^*, \beta^*) = 0 \;\;\; i = 1, \cdots, l \\
\alpha_i^*g_i(\omega^*) = 0 \;\;\; i = 1, \cdots, k \\
g_i(\omega^*) \le 0 \;\;\; i = 1, \cdots, k \\
\alpha_i^* \ge 0 \;\;\; i = 1, \cdots, k 
\end{aligned}
$$
Also, we have:
$$
\omega^*, \alpha^*, \beta^* \text{are the solution to the dual and primal problem correspondingly} \\
\iff \omega^*, \alpha^*, \beta^* \text{satisfy the KKT conditions}
$$

We should draw attention to the third condition, **$\alpha_i^*g_i(\omega^*) = 0$**, which is called the 
**KKT dual complementarity** condition. Specifically, it implies that if $\alpha_i^* > 0$, then $g_i(\omega^*) = 0$.

### Optimal Margin Classifiers ###

Previous in the session of functional and geometric margin, we posed the following primal optimization problem for 
finding the optimal margin classifier:
$$
\begin{aligned}
\min_{\gamma, \omega, b} & \frac{1}{2} \|\omega\|^2 \\
s.t. & y_i(\omega^Tx_i + b) \ge 1, \;\;\; i = 1, \cdots, m 
\end{aligned}
$$
Because in generalized Lagrangian we always have $\le$ inequality, we can write the constraints as:
$$
g_i(\omega) = - y_i(\omega^Tx_i + b) + 1 \le 0
$$
We have one such constraint for each training example. Note that from the KKT dual complementarity condition, we will 
have $\alpha_i > 0$ only for the training examples that have functional margin exactly equal to 1. Those points are
called the **support vectors**

Now we construct the Lagrangian for our optimization problem:
$$
\mathcal{L}(\omega, b, \alpha) = \frac{1}{2}\|\omega\|^2 - \sum_{i=1}^m \alpha_i[y_i(\omega^Tx_i + b) - 1]
$$
Let's find the dual form first.

#### Why dual form not primal form ####

dual and primal problems are two different ways to solve the optimization problem right above. If we try the primal
form first, then we have to try to find out the $\theta_{\mathcal{P}}(\omega)$ which is the maximum of $\mathcal{L}$ over 
$\alpha$ for a given $\omega$.
It is easy to find that this maximization cannot be solved by taking the derivative with respect to $\alpha$

Then we are thinking if we can solve the dual form under specific conditions, then the solution to dual form will also be
the solution to the primal problem. So that is why we are considering the dual form now.

#### Dual form to primal form ####

We need to minimize $\mathcal{L}(\omega, b, \alpha)$ with respect to $\omega$ and $b$
for fixed $\alpha$ to get $\theta_{\mathcal{D}}$. So we can have:
$$
\bigtriangledown_{\omega} \mathcal{L}(\omega, b, \alpha) = \omega - \sum_{i=1}^m\alpha_iy_ix_i = 0
$$
This implies that 
$$
\omega = \sum_{i=1}^m \alpha_iy_ix_i
$$
As for the derivative with respect to $b$, we obtain
$$
\frac{\partial}{\partial b} \mathcal{L}(\omega, b, \alpha) = \sum_{i=1}^m\alpha_iy_i = 0
$$

If we take the definition of $\omega =\sum_{i=1}^m \alpha_iy_ix_i$ and plug that back into the $\mathcal{L}$,
also replace $\sum_{i=1}^m \alpha_iy_i$ with 0. Then we have:
$$
\begin{aligned}
\mathcal{L}(\omega, b, \alpha) & = \frac{1}{2}\omega^T\omega - \sum_{i=1}^m\alpha_iy_i\omega^Tx_i - 
b\sum_{i=1}^m\alpha_iy_i + \sum_{i=1}^m\alpha_i \\
& = \sum_{i=1}^m\alpha_i +\omega^T(\frac{1}{2}\omega - \sum_{i=1}^m\alpha_iy_ix_i) \\
& = \sum_{i=1}^m\alpha_i +\omega^T(\frac{1}{2}\omega - \omega) \\
& = \sum_{i=1}^m\alpha_i - \frac{1}{2}\omega^T\omega \\
& = \sum_{i=1}^m\alpha_i - \frac{1}{2}(\sum_{i=1}^m\alpha_iy_ix_i)^T(\sum_{j=1}^m\alpha_jy_jx_j) \\
& = \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_j y_iy_j x_i^Tx_j
\end{aligned}
$$

Then the next step is to maximize the $\mathcal{L}$ in the above form. Putting this together with constraints $\alpha_i \ge 0$
we obtain the following dual optimization problem:
$$
\begin{aligned}
\max_{\alpha} & W(\alpha) = \sum_{i=1}^m\alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_j y_iy_j 
\langle x_i, x_j \rangle \\
s.t. & \alpha_i \ge 0 \;\;\; i = 1, \cdots, m \\
& \sum_{i=1}^m \alpha_iy_i = 0
\end{aligned}
$$
More important, 
the conditions for $p^* = d^*$
- $f$ and $g_i$'s are convex
- $h_i$'s are affine, which means there exists $a_i$ and $b_i$ so that $h_i(\omega) = a_i^T\omega + b_i$. Affine means the same
thing as linear, except that we also allow the extra intercept term $b_i$
- constraints $g_i$ are strictly feasible, which means there exists some $\omega$ so that $g_i(\omega) < 0$ for all $i$.

and the KKT conditions: 
```
(I should add more intuition here!!!)
```
- No matter what $\alpha$ we found, $\omega = \sum_{i=1}^m \alpha_i y_i$ is the solution to 
$\bigtriangledown_{\omega} \mathcal{L}(\omega, b, \alpha) = 0$. Same for the $b$. This makes the first KKT condition to be valid.
- Second KKT condition we do not have here since we do not have equality constraint. Wen only have $\alpha$ here.
- $g_i(\omega) = - y_i(\omega^Tx_i + b) + 1 \le 0$ is satisfied here.
- We have $\alpha_i \ge 0$
- $\alpha_ig_i(\omega) = 0$ is satisfied when construct the Lagrangian for our optimization problem 

to hold are indeed satisfied in this optimization problem. 

Suppose we are indeed able to solve the above maximization problem with respect to $\alpha$, then we can use 
$\omega = \sum_{i=1}^m \alpha_i y_ix_i$ to go back and find the $\omega^*$. Using $\alpha^*$ and $\omega^*$, then we solve the $b^*$
by 
$$
b^* = \arg\min_{b}\mathcal{L}(\omega^*, b, \alpha^*) = -\frac{\max_{i:y_i=-1}\omega^{*T}x_i + \min_{i:y_i=1}\omega^{*T}x_i}{2}
$$
```
I should add more intuition here!!!
```

Moreover, if we take a more careful look at the $\omega = \sum_{i=1}^m \alpha_i y_ix_i$. Suppose we have fit our model's 
parameters to a training set, and now wish to make a prediction at a new point input $x_{\text{new}}$. We would then calculate 
$\omega^Tx_{\text{new}} +b$:
$$
\begin{aligned}
\omega^Tx_{\text{new}} + b = & \left( \sum_{i=1}^m \alpha_iy_ix_i\right)^T x_{\text{new}} + b \\
& = \sum_{i=1}^m \alpha_iy_i \langle x_i, x_{\text{new}}\rangle + b
\end{aligned}
$$

Hence, if we have found the $\alpha_i$'s, in order to make a prediction, we have to calculate a quantity that depends only on 
the inner product between $x_{\text{new}}$ and the points in the training set. And we know that the $\alpha_i$'s will all be
zero except for the support vectors. Thus many of the terms in the sum above will be zero, and we only need to find the inner
products between $x_{\text{new}}$ and the support vectors.

### Kernel ###

As we noticed in the previous session, only the inner products between $x_{\text{new}}$ and the support vectors is need for 
prediction of new observation. So applying Kernels to our classification problem will be able to efficiently learn in very 
high dimensional spaces.

#### Feature mapping ####

First of all, let's talk a little about **attributes** and **features**.
- **attributes**: is the original input value, like $x$
- **features**: is the original input $x$ is mapped to some new set of quantities that are then passed to the model, like
$x$, $x^2$, $x^3$
We also let $\phi$ denote the **feature mapping** which maps from the attributes to the features. For example, we can 
have 
$$\phi(x) = [x \;\;\; x^2 \;\;\; x^3]^T$$

So rather than applying SVM using the original input attributes $x$, we may instead want to learn using some features 
$\phi(x)$. To do so, we simply need to go over our previous algorithm and replace $x$ everywhere in it with $\phi(x)$.

#### Kernel and mapping ####

Since the prediction of new point can be written entirely in terms of the inner products 
$\langle x_i, x_{\text{new}}\rangle$, this means that we would replace all those inner products with 
$\langle \phi(x_i), \phi(x_{\text{new}})\rangle$. Specifically, given a feature mapping $\phi$, we define the 
**corresponding Kernel** to be
$$
K(x_i, x_{\text{new}}) = \phi(x_i)^T\phi(x_{\text{new}})
$$
Then everywhere we previously had $\langle x_i, x_{\text{new}}\rangle$ in our algorithm, we could simply replace it
with $K(x_i, x_{\text{new}})$, and the algorithm would now be learning using the features $\phi$.

Now, given $\phi$, we could easily compute $K(x_i, x_{\text{new}})$ by finding $\phi(x_i)$ and $\phi(x_{\text{new}})$
and taking their inner product. On the other hand, for a given $K(\cdot, \cdot)$, we also could find a $\phi$ which is
one-to-one correspondent to it. So there is a one-to-one correspondence between a feature mapping to a Kernel. What is
more interesting is that often, $K(x_i, x_{\text{new}})$ may be very inexpensive to calculate, even though $\phi(x)$
itself may be very expensive to calculate, perhaps because it is an extremely high dimensional vector. 

In such settings, by using in SVM an efficient way to calculate kernel $K(\cdot, \cdot)$, we can get SVM to learn 
in the high dimensional feature space given by $\phi$ but without ever having to calculate the $\phi(x)$.
For example, suppose $x,z \in \mathbb{R}^d$, and we define the kernel as:
$$
K(x,z) = (x^Tz)^2
$$
We can also write this as:
$$
\begin{aligned}
K(x,z) & = \left( \sum_{i=1}^d x_iz_i \right)\left( \sum_{i=1}^d x_iz_i \right) \\
& = \sum_{i=1}^d \sum_{j=1}^d (x_ix_j)(z_iz_j)
\end{aligned}
$$
Thus we see that $K(x,z) =  \phi(x)^T\phi(z)$, where the feature mapping $\phi$ is (shown here for the case of $d=3$):
$$
\phi(x) = [x_1x_1 \;\; x_1x_2 \;\; x_1x_3 \;\; x_2x_1 \;\; x_2x_2 \;\; x_2x_3 \;\; x_3x_1 \;\; x_3x_2 \;\; x_3x_3]^T
$$
So the $\phi(x)$ is a mapping: $\mathbb{R}^d \to \mathbb{R}^{d^2}$.
Note that whereas calculating the high-dimensional $\phi(x)$ requires $O(n^2)$ time, however finding $K(x,z)$ takes
only $O(n)$ time.

In summary, Kernel methods owe their name to the use of kernel functions, which enable them to operate in a 
high-dimensional, implicit feature space without ever computing the coordinates of the data in that 
space, but rather by simply computing the inner products between the images of all pairs of data in the
feature space. This operation is often computationally cheaper than the explicit computation of the 
coordinates. This approach is called the "kernel trick". 

#### Different view of Kernel ####

From a different view of kernels, if $\phi(x)$ and $\phi(z)$ are close together, then we might expect $K(x,z) = \phi(x)^T\phi(z)$
to be large. Conversely, if $\phi(x)$ and $\phi(z)$ are far apart, say nearly orthogonal to each other, then 
$K(x,z) = \phi(x)^T\phi(z)$ will be small. So we can think of $K(x,z)$ as some measurement of how similar are
$\phi(x)$ and $\phi(z)$, or of how similar are $x$ and $z$.

Since there is a one-to-one correspondence between a feature mapping to a Kernel, we can either define a feature mapping
and then find the corresponding Kernel, or define a Kernel then find the corresponding mapping. But most of time
people prefer the second way, given some function $K$, how can we tell if it is a valid kernel, which means how can we 
tell if there is some feature mapping $\phi$ corresponds to it?

#### Kernel matrix ####

Suppose for now that $K(\cdot, \cdot)$ is indeed a valid kernel corresponding to some feature mapping $\phi$. We define a 
square, m-by-m matrix $K$ that its $(i,j)$-entry is given by $K_{ij} = K(x_i, x_j)$. This matrix is called the **Kernel matrix**.
- Kernel matrix must be symmetric since $K_{ij} = K(x_i, x_j) = \phi(x_i)^T\phi(x_j) =  \phi(x_j)^T\phi(x_x) = K(x_j, x_i) = K_{ji}$ 
- Kernel matrix is positive semi-definite, since for any vector $z$, we have 
$z^TKz = \sum_i\sum_j z_iK_{ij}z_j = \sum_k\left(\sum_i z_i\phi_k(x_i)\right)^2 \ge 0$, $k$ here is the index for $k$-th 
coordinate of the vector $\phi(x)$.

**Theorem (Mercer)**
Let $K: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ be given. Then for $K$ to be a valid kernel, it is necessary and
sufficient that for any ${x_i, \cdots, x_m}$, the corresponding kernel matrix is symmetric positive semi-definite.

## Non-linearly Separable ##

### "Slack" Variables ###

Previously, we introduced how to separate points by linearly separable hyperplane. But it is possible
that the large margin solution is better even though one constraint is violated.

So we permit to have functional margin less than 1 for some cases, such that 
$$y_i(\omega^Tx_i + b) \ge 1 - \xi_i$$
$$\xi_i \ge 0$$ 
This $\xi_i$ are called slack variables. For the support vectors, the functional 
margin is 
$$\gamma_i = y_i(\omega^Tx_i + b) = 1,$$ 
which means $\xi_i = 0$, and distance from support vectors to classification hyperplane is $\frac{1}{\|\omega\|}$.
Originally, within either class, distance from any point to the classification hyperplane is larger or equal to 
the $\frac{1}{\|\omega\|}$. But now since we permit have some cases that functional margin is less than 1, we will
have some cases that the distance from those points to the classification hyperplane are less than 1 or maybe 
even larger than 1 but on the other side. So in other words, there are two different situations for slack variable.

#### Margin violation ####

For this type of situation, we have **$0 < \xi_i \le 1$**. This means the distance between this type of points
and the hyperplane $\omega^Tx + b$ is:
$$ 0 \le \frac{1 - \xi_i}{\|\omega\|} < \frac{1}{\|\omega\|}$$
The above distance greater or equal to zero means this point is still on the correct side of classification 
hyperplane. On the other hand, it should smaller than $\frac{1}{\|\omega\|}$, which is the distance from 
support vector.

In summary, margin violation means we will permit a correct classification point but with smaller margin than
support vectors. One thing to keep in mind that the distance we are talking about here do not have a direction.
Which means we are talking about the distance for a point from a given class to the hyperplane. Later on we 
will see a negative distance, that means the distance is from the other side.

#### Misclassified ####

For this type of situation, we have **$\xi_i > 1$**. This means the distance between this type of points and the
hyperplane $\omega^Tx + b$ is:
$$
\frac{1 - \xi_i}{\|\omega\|} < - \frac{1}{\|\omega\|}
$$
The magnitude of this distance is actually greater than $\frac{1}{\|\omega\|}$, which is because the distance
is larger than the support vectors. However the negative sign with the distance means this point is misclassified
on the other side.

### Regularization ###

Since we permit margin violation or misclassified points in our sample, we should add penalty on those points.
One thing should be noticed that if the $\xi_i$ is sufficiently large, then constraint for every point can be
satisfied.

$$ 
\begin{aligned}
& \min_{\gamma, \omega, b} \frac{1}{2}||\omega||^2 + C\sum_{i=1}^{m} \xi_i \\
s.t. & y_i(\omega^Tx_i + b) \ge 1 - \xi_i \\
& \xi_i \ge 0
\end{aligned}
$$

C is the regularization parameter, 
- If C is small, then all constraints are easily to be ignored. It means we can have relatively large $\xi_i$
and to have a large margin with many cases of margin violation and misclassification.
- If C is large, then all constraints are hard to be ignored. It means we can have small $\xi_i$ and to have a
narrow margin in order to avoid margin violation or misclassification.
- If C is $\infty$, then all constraints are enforced to be satisfied, which called hard margin. In this extreme
case, we do not have soft margin solution, and which goes back to original SVM.

The value of C can also be viewed as a way to trade off between overfitting (high variance) and underfitting (high 
bias). 
- If C is too small, it will cause the overfitting problem
- If C is too large, it will then cause the underfitting problem.

Then the optimization problem can be reformed using **Lagrange multipliers** method. Let us define 
the **Lagrangian** to be:

$$
\begin{aligned}
\mathcal{L}(\omega, b, \xi, \alpha, r) = \frac{1}{2}\omega^T\omega + C\sum_{i=1}^m\xi_i - 
\sum_{i=1}^m\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] - \sum_{i=1}^mr_i\xi_i
\end{aligned}
$$

Here $\alpha_i$ and $r_i$ are our Lagrange multipliers.
All optimal solutions must satisfy Karush-Kuhn-Tucker (KKT) conditions:

$$
\begin{aligned}
\frac{\partial}{\partial \omega}\mathcal{L} =  \omega - \sum_{i=1}^m \alpha_i y_i x_i = 0 \\
\frac{\partial}{\partial b}\mathcal{L} = \sum_{i=1}^m \alpha_i y_i = 0 \\
\frac{\partial}{\partial \xi_i}\mathcal{L} = C - \alpha_i - r_i = 0 & \;\;\; i = 1, 2, \cdots, m \\
\alpha_i[y_i(x^Tx + b) - 1 + \xi_i] = 0 & \;\;\; i = 1, 2, \cdots, m \\
r_i\xi_i = 0 & \;\;\; i = 1, 2, \cdots, m \\
\alpha_i \ge 0 & \;\;\; i = 1, 2, \cdots, m \\
r_i \ge 0 & \;\;\; i = 1, 2, \cdots, m \\
-\xi_i \le 0 & \;\;\; i = 1, 2, \cdots, m \\
-[y_i(x^Tx + b) - 1 + \xi_i] \le 0 & \;\;\; i = 1, 2, \cdots, m
\end{aligned}
$$

First we look at the third condition. It can be rewrote as $\alpha_i = C - r_i$, recall that $r_i \ge 0$.
So we can get the condition for $\alpha_i$ as:
$$0 \le \alpha_i \le C$$

The optimality conditions are both necessary and sufficient. If we have
$C$, $\xi$, b, $\alpha$, and r satisfying the above conditions, we know that they represent optimal 
solutions to the primal and dual problems.

#### In summary ####

The regularization problem can be written as following:

We plug into $\mathcal{L}(\omega, b, \xi, \alpha, r)$ with $\omega = \sum_{i=1}^m \alpha_i y_i x_i$,
$\sum_{i=1}^m \alpha_i y_i = 0$, and $C - \alpha_i - r_i = 0$

$$
\begin{aligned}
\mathcal{L}(\omega, b, \xi, \alpha, r) & = \frac{1}{2}\omega^T\omega + \sum_{i=1}^m\alpha_i - \sum_{i=1}^m\alpha_iy_i
\omega^Tx_i - \sum_{i=1}^m\alpha_iy_ib + \sum_{i=1}^m(C - \alpha_i -r_i)\xi_i \\
& = \sum_{i=1}^m \alpha_i + \frac{1}{2}\omega^T\omega - \omega^T\omega \\
& = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_iy_j \langle x_i, x_j \rangle \\
& = e^T\alpha - \frac{1}{2}\alpha^TQ\alpha
\end{aligned}
$$
where $e$ is the vector of all ones, $\alpha$ is the vector of all $\alpha_i$. $Q_{ij} = y_iy_jK(x_i,x_j)$.
Then the optimization problem is:
$$
\begin{aligned}
\max_{\alpha} & W(\alpha) = \sum_{i=1}^m\alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_j y_iy_j 
\langle x_i, x_j \rangle \\
s.t. & 0 \le \alpha_i \le C \;\;\; i = 1, \cdots, m \\
& \sum_{i=1}^m \alpha_iy_i = 0
\end{aligned}
$$

Also among all conditions, the KKT dual-complementarity conditions are:
$$
\begin{aligned}
\alpha_i[y_i(x^Tx + b) - 1 + \xi_i] = 0 & \;\;\; i = 1, 2, \cdots, m \\
r_i\xi_i = 0 & \;\;\; i = 1, 2, \cdots, m \\
\end{aligned}
$$
Then, we can have 
- If $\alpha_i = 0$, then $y_i(x^Tx + b) - 1 + \xi_i > 0$, according to the third condition of KKT
$r_i = C \Rightarrow \xi_i = 0 \Rightarrow y_i(x^Tx + b) > 1$
- If $\alpha_i = C$, then according to the third condition of KKT, $r_i = 0 \Rightarrow \xi_i >0$, 
$y_i(x^Tx + b) - 1 + \xi_i = 0 \Rightarrow y_i(x^Tx + b) < 1$
- If $0 < \alpha_i < C$, then $y_i(x^Tx + b) - 1 + \xi_i = 0$, and $r_i \ne 0 \Rightarrow \xi_i = 0$.
So $y_i(x^Tx + b) = 1$

## SMO Algorithm ##

### Coordinate Ascent ###

Consider trying to solve the unconstrained optimization problem
$$
\max_{\alpha}W(\alpha_1, \alpha_2,\cdots,\alpha_m)
$$
we are going to consider an algorithm named **coordinate ascent**:

$$
\begin{aligned}
&\text{Loop unitl convergence:} \{ \\
&\;\;\;\text{For} \; i = 1, \cdots, m \{ \\
&\;\;\;\;\;\; \alpha_i = \arg\max_{\alpha_i}W(\alpha_1,\cdots,\alpha_{i-1},\hat\alpha_i, \alpha_{i+1}, 
\cdots,\alpha_m) \\
&\;\;\;\} \\
&\}
\end{aligned}
$$

It is easy to find that in each iteration, we update $W$ with respect to one of the $\alpha_i$, then
we have a new $W$ value with new $\hat \alpha_i$ plugged in. And then we update the new 
$W(\hat \alpha_i)$ with respect to the next $\alpha_{i+1}$ until all $\alpha_i$ have been updated. Those
all happened for every iteration.

Coordinate descent is a non-derivative optimization algorithm. To find a local minimum of a function,
we do **line search along one coordinate direction**, which is one $\alpha_i$, at the current point in each 
iteration. We use different coordinate directions cyclically throughout the procedure.

#### Gradient ascent ####

Gradient ascent is a little different than coordinate ascent. 
To find a local minimum of a function using gradient descent, we take steps proportional to the negative 
of the gradient (or of the approximate gradient) of the function at the current point. If instead we take 
steps proportional to the positive of the gradient, one approaches a local maximum of that function; the 
procedure is then known as gradient ascent.

In coordinate ascent, we update all $\alpha_i$ one by one in each iteration. In other words, we updated 
the objective function by plugging in the new estimate $\hat \alpha_i$.

On the other hand, gradient ascent will update all $\alpha_i$ simultaneously within each iteration.
$$
\begin{aligned}
&\text{Loop unitl convergence:} \{ \\
&\;\;\;\;\;\; \alpha_1 = \alpha_1 + \gamma \frac{\partial}{\partial \alpha_1}W \\
&\;\;\;\;\;\; \cdots \\
&\;\;\;\;\;\; \alpha_d = \alpha_d + \gamma \frac{\partial}{\partial \alpha_d}W \\
&\}
\end{aligned}
$$
where $\gamma$ is the  a step size (sometimes called the learning rate in machine learning). Here the dimension
of $\alpha$ is same as the $x_i$, previously in SVM we have dimension of $\alpha$ is $m$ which is the number 
of data point.

#### Stochastic gradient ascent ####

Here is an extension of gradient ascent which called stochastic gradient ascent. Back to gradient ascent,
the gradient $\partial W / \partial\alpha_i$ should be calculated based on all data $x_i$, like in the linear
regression, gradient is 
$$
\frac{\partial}{\partial\alpha_j} W = \frac{1}{m}\sum_{i=1}^m (W(x_i) - y_i)x_{ij}
$$
Every time when we update one $\alpha_i$, we have to scan the whole dataset into memory. When the dataset is 
large, this is extremely inefficient.

Another method which can overcome this problem is called stochastic gradient ascent. Still take the linear 
regression as example. Previously, the objective cost function is a summation of error square for each data
point. So each data point we have an objective term: $W_i = \frac{1}{2}(W(x_i) - y_i)^2$, and the gradients are
$$
\frac{\partial}{\partial\alpha_j} W_i = (W(x_i) - y_i)x_{ij}
$$
So we have an objective function for each data point. Then for each objective function, we can update every
$\alpha_i$.
$$
\begin{aligned}
& \text{First randomly shuffle dataset} \\
& \text{Loop unitl convergence:} \{ \\
& \;\;\;\;\;\; \text{For} i = 1, \cdots, m \{ \\
& \;\;\;\;\;\;\;\;\; \alpha_1 = \alpha_1 + \gamma \frac{\partial}{\partial \alpha_1} W_i \\ 
& \;\;\;\;\;\;\;\;\; \cdots \\
& \;\;\;\;\;\;\;\;\; \alpha_d = \alpha_d + \gamma \frac{\partial}{\partial \alpha_d} W_i \\
& \;\;\;\;\;\; \} \\
& \}
\end{aligned}
$$
Learning rate $\gamma$ is typically held constant, but can also slowly decreased over iteration time if we want
$\alpha$ to converge, like $\gamma = \frac{constant1}{iterationtime + constant2}$.

### SMO Procedure ###

The (dual) optimization problem that we want to solve:
$$
\begin{aligned}
\min_{\alpha} & W(\alpha) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_j y_iy_j 
\langle x_i, x_j \rangle - \sum_{i=1}^m\alpha_i \\
s.t. & 0 \le \alpha_i \le C \;\;\; i = 1, \cdots, m \\
& \sum_{i=1}^m \alpha_iy_i = 0
\end{aligned}
$$
Let's say we have set of $\alpha_i$'s that satisfy the two constrints. Then our plan is to use coordinate descent 
to solve the optimization problem. Recall that in the corrdinate ascent/descent method, we only update one $\alpha_i$
and fix the rest of $\alpha$ to be constant each time. Here we have one problem if we follow the original coordinate
descent. Becasue of the linear constraint of all $\alpha_i$, a linear combination of any two $\alpha_i$ will be a 
constant. So we have to update two $\alpha_i$ in each step of coordinate descent.

Every step of SMO, we selection some pair of $\alpha_i$ and $\alpha_j$ to update next, and then reoptimize $W(\alpha)$
with respect to $\alpha_i$ and $\alpha_j$, while holding all the other $\alpha$'s fixed.

#### How it works: ####

Later on we will discuss how to choose these two $\alpha_i$'s, here let's assume we want to work on $\alpha_1$ and 
$\alpha_2$. So we fix $\alpha_3, \cdots, \alpha_m$, also we assume that the initial value of $\alpha$ satisfy
$$
\sum_{i=1}^m \alpha_iy_i = 0
$$
Then we will have
$$
\alpha_1y_1 + \alpha_2y_2 = - \sum_{i=3}^m y_i\alpha_i
$$
Here $\xi$ is a constant. Suppose $\alpha_1$ and $\alpha_2$ are the updated value in $k$'th step, and 
$\alpha_1^*$ and $\alpha_2^*$ are the value in $(k-1)$'th step, and we assume that all $\alpha_i^*$ 
satisfy the two constraints in the original problem. So we are trying to update from 
$\alpha_1^*$ and $\alpha_2^*$ to $\alpha_1$ and $\alpha_2$. Still we have following linear constraint for
old $\alpha$'s:
$$
\alpha_1y_1 + \alpha_2y_2 = \alpha_1^*y_1 + \alpha_2^*y_2 
$$
So, our SMO procedure basically is trying to calculate $\alpha_1$ and $\alpha_2$ given $\alpha_1^*$ and
$\alpha_2^*$ in each step. Let $s = y_1y_2$, and $y_1y_1 = y_2y_2 =1$. Then multiply $y_1$ on each side
of above constraint we have:
$$
\alpha_1 + s\alpha_2 = \alpha_1^* + s\alpha_2^* = -y_1 \sum_{i=3}^m y_i\alpha_i^* = \xi
$$
We also have:
$$
\alpha_1 = \xi - s\alpha_2
$$
Further more, we define the following variables:
$$
u_1 = \sum_{i=1}^m y_i\alpha_i^*K(x_i,x_1) + b^* \;\;\;\; \text{and} \;\;\;\;
u_2 = \sum_{i=1}^m y_i\alpha_i^*K(x_i,x_2) + b^* 
$$
It is not hard to notice that $u_1$ and $u_2$ are the estimated value for $x_1$ and $x_2$ based on the
current parameter estimates $\alpha_1^*$, $\alpha_2^*$, and $b^*$. $K(\cdot, \cdot)$ is basically the
kernel function, later on we will use $K_{ij}$ to represent the $ij$'th element in the kernel matrix. 

Since we can use $\alpha_2$ to replace $\alpha_1$, in objective function $W$ can be rewrote as a function
with respect to $\alpha_2$ only, and any other terms that are not related to $\alpha_2$ can be treated
as constant:
$$
\begin{aligned}
& W(\alpha) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i\alpha_j y_iy_j \langle x_i, x_j \rangle -
\sum_{i=1}^m\alpha_i \\
=& \frac{1}{2}K_{11}\alpha_1^2y_1^2 + \frac{1}{2}K_{22}\alpha_2^2y_2^2 + sK_{12}\alpha_1\alpha_2 + 
\sum_{i=3}^m \alpha_1\alpha_i^*y_1y_iK_{1i} + \sum_{i=3}^m \alpha_2\alpha_i^*y_2y_iK_{2i} -\alpha_1
-\alpha_2 + Constant \\
=& \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + sK_{12}\alpha_1\alpha_2 + \alpha_1y_1\
\upsilon_1 + \alpha_2y_2\upsilon_2 - \alpha_1 - \alpha_2 + Constant \\
=& \frac{1}{2}K_{11}(\xi - s\alpha_2)^2 + \frac{1}{2}K_{22}\alpha_2^2 + sK_{12}\alpha_2(\xi - s\alpha_2)+
y_1(\xi - s\alpha_2)\upsilon_1 + y_2\alpha_2\upsilon_2 - \xi + s\alpha_2 - \alpha_2 + Constant \\
\end{aligned}
$$
where
$$
\begin{aligned}
\upsilon_1 = & \sum_{j=3}^m y_j\alpha_j^*K_{1j} = u_1 - b^* - y_1\alpha_1^*K_{11} - y_2\alpha_2^*K_{12}\\
\upsilon_2 = & \sum_{j=3}^m y_j\alpha_j^*K_{2j} = u_2 - b^* - y_1\alpha_1^*K_{12} - y_2\alpha_2^*K_{22}\\
Constant = & \sum_{i=3}^m \sum_{j=3}^m \alpha_i^* \alpha_j^* y_iy_jK_{ij} - \sum_{i=3}^m \alpha_i
\end{aligned}
$$
Now it is very clear that our objective $W(\alpha_2)$ is a quadratic function with respect to $\alpha_2$.
One more thing should be recalled that the $\alpha_2$ can only be with in $(0, C)$ and $\alpha_1$ is in 
$(0, C)$, $\alpha_1 + s\alpha_2 = \xi$. And we know that the 
optimization of the $W(\alpha_2)$ will only be at either the endpoint or the $\alpha_2$ such that 
$W'(\alpha_2) = 0$. So let us first have a look at what is the function value at the endpoints.

There are two different situations for the endpoints of $\alpha_2$:

**When $y_1$ and $y_2$ are different** 
![y1 and y2 are different](./figures/smo1.png)

As in the figure, possible value of $\alpha_1$ and $\alpha_2$ first should be located in the dash square.
Plus $\alpha_1$ and $\alpha_2$ should be on the line $\alpha_1 - \alpha_2 = \xi$. If the line is the 
upper one, then we can see $0 < \alpha_1 < C-\xi$ and $\xi < \alpha_2 < C$. Or if is the lower line, 
then we have $\xi < \alpha_1 < C$ and $0 < \alpha_2 < C-\xi$. And we do not care about $\alpha_1$, so boundary 
for $\alpha_2$ is (in order to match the graph, we changed the sign in front of $\alpha_1$ and $\alpha_2$ in 
$\alpha_1 - \alpha_2 = \xi$):
$$
L = \max(0, \alpha_2^* - \alpha_1^*) \;\;\;\; U = \min(C, C + \alpha_2^* - \alpha_1^*)
$$

**When $y_1$ and $y_2$ are same**
![y1 and y2 are same](./figures/smo2.png)

Same as before, possible value of $\alpha_1$ and $\alpha_2$ first should be located in the dash square.
Plus $\alpha_1$ and $\alpha_2$ should be on the line $\alpha_1 + \alpha_2 = \xi$. If the line is the 
upper one, then we can see $\xi - C < \alpha_1 < C$ and $\xi - C< \alpha_2 < C$. Or if is the lower line, 
then we have $0 < \alpha_1 < \xi$ and $0 < \alpha_2 < \xi$. And we do not care about $\alpha_1$, so boundary 
for $\alpha_2$ is (in order to match the graph, we changed the sign in front of $\alpha_1$ and $\alpha_2$ in 
$\alpha_1 - \alpha_2 = \xi$):
$$
L = \max(0, \alpha_2^* + \alpha_1^* - C) \;\;\;\; U = \min(C, \alpha_2^* + \alpha_1^*)
$$

Now is the time for first derivative of $W$ with respect to $\alpha_2$.
$$
\begin{aligned}
& \frac{\mathrm{d}W}{\mathrm{d}\alpha_2} = -sK_{11}(\xi -s\alpha_2) + K_{22}\alpha_2 -2K_{12}\alpha_2 + sK_{12}\xi +s -1 +
y_2\upsilon_2 - sy_1\upsilon_2 = 0 \\
\Leftrightarrow & \alpha_2(K_{11} + K_{22} - 2K_{12}) = s(K_{11} - K_{12})\xi + y_2(\upsilon_1 - \upsilon_2) + 1 - s
\end{aligned}
$$
where
$$
\begin{aligned}
RHS & = s(K_{11} - K_{12})(\alpha_1^* + s\alpha_2^*) + y_2(u_1 - u_2 - y_1\alpha_1^*K_{11} - y_2\alpha_2^*K_{12} + 
y_1\alpha_1^*K_{12} + y_2\alpha_2^*K_{22}) + 1 - s\\
& = K_{11}\alpha_2^* - K_{12}\alpha_2^* + y_2(u_1 - u_2) - K_{12}\alpha_2^* + K_{22}\alpha_2^* + y_2(y_2 - y_1) \\
& = \alpha_2^*(K_{11} + K_{22} - 2K_{12}) + y_2(u_1 - u_2 + y_2 - y_1)
\end{aligned}
$$
We introduce a new notation here:
$$
\eta = K_{11} + K_{22} - 2K_{12}
$$
So the update $\alpha_2$ for the case that optimization is at the derivative zero point can be written down as:
$$
\alpha_2 = \alpha_2^* + \frac{y_2(E_1 - E_2}{\eta}
$$
Finally, combining the situation at the endpoint of $\alpha_2$, we have our final version of updating:
\[ \alpha_2^{new} = \left\{
  \begin{array}{l l}
    U  & \quad \text{if} \alpha_2 \geq U \\
    \alpha_2  & \quad \text{if} L < \alpha_2 < U \\
    L  & \quad \text{if} \alpha_2 \leq L
  \end{array} \right.\]
and for $\alpha_1$:
$$
\alpha_1^{new} = \alpha_1 + s(\alpha_2 - \alpha_2^{new})
$$
Let us have a deeper look at this $\eta$ variable. For most of situation, $\eta$ should be non-negative, but sometimes it does
not. If the kernel function does not satisfy the Mercer Theorem, then the objective function may not be non-negative, then 
$\eta$ maybe negative. Even the kernel function is valid, if there are two same training sample in X, then $\eta$ could be 0.
Good news is that SMO algorithm is still valid for those situation. It turns out that $\eta$ is nothing but the second derivative
of $W(\alpha_2)$ with respect to $\alpha_2$. If $\eta < 0$, there is no minimum value for $W$, so the minimum will be at 
endpoint. If $\eta = 0$, then $W$ is a monotonous function, minimum will also be at endpoint OF $\alpha_2$. We will find the 
corresponding $\alpha_1$ for endpoint of $\alpha_2$, and then plug them into $W$.
$$
\begin{aligned}
L_1 = \alpha_1^* + s(\alpha_2^* - L) \\
U_1 = \alpha_1^* + s(\alpha_2^* - U) \\
\end{aligned}
$$
$L_1$ and $U_1$ are the corresponding $\alpha_1$. 
$$
\begin{aligned}
& W(L, L_1) = [\frac{1}{2}K_{11}L_1^2 + \frac{1}{2}K_{22}L^2 + sK_{12}LL_1] + [L_1(y_1\upsilon_1 - 1)] + 
[L(y_2\upsilon_2 - 1)] + Constant \\
& \text{2ed square brackets} = [L_1(y_1(u_1 - b^*-y_1\alpha_1^*K_{11} - y_2\alpha_2^*K_{12}) - 1)] \\
& = [L_1(y_1(E_1+y_1-b^*-y_1\alpha_1^*K_{11} - y_2\alpha_2^*K_{12}) - 1)] \\
& = L_1(y_1(E_1 - b^*) -\alpha_1^*K_{11} - s\alpha_2^*K_{12}) \\
& \text{3rd square brackets} = [L(y_2(u_2 - b^* - y_1\alpha_1^*K_{12} - y_2\alpha_2^*K_{22}) - 1)] \\
& = L(y_2(E_2 - b^*) - y_1\alpha_1^*K_{12} - y_2\alpha_2^*K_{22})
\end{aligned}
$$
And same for the upper limit:
$$
\begin{aligned}
& W(U, U_1) = [\frac{1}{2}K_{11}U_1^2 + \frac{1}{2}K_{22}U^2 + sK_{12}UU_1] + [U_1(y_1\upsilon_1 - 1)] + 
[U(y_2\upsilon_2 - 1)] + Constant \\
& \text{2ed square brackets} = [U_1(y_1(u_1 - b^*-y_1\alpha_1^*K_{11} - y_2\alpha_2^*K_{12}) - 1)] \\
& = [U_1(y_1(E_1+y_1-b^*-y_1\alpha_1^*K_{11} - y_2\alpha_2^*K_{12}) - 1)] \\
& = U_1(y_1(E_1 - b^*) -\alpha_1^*K_{11} - s\alpha_2^*K_{12}) \\
& \text{3rd square brackets} = [U(y_2(u_2 - b^* - y_1\alpha_1^*K_{12} - y_2\alpha_2^*K_{22}) - 1)] \\
& = U(y_2(E_2 - b^*) - y_1\alpha_1^*K_{12} - y_2\alpha_2^*K_{22})
\end{aligned}
$$

OK, so far we have covered how to minimize the objective function with respect to two $\alpha$'s.
THe problem before this minimization is how to choose these two $\alpha$ in order to speed up the procedure.
So we first find a $\alpha_2$ which will be updated, then choose a $\alpha_1$ which can maximize the 
$\|E_1 - E_2\|$. Here is the two steps.
- outter loop: this loop will loop over samples whose $\alpha$ is $0 < \alpha_i < C$ or all the samples.
We first loop over the samples whose $\alpha$ is $0 < \alpha_i < C$ because those samples are support
vectors, and support vectors' $\alpha_i$ are more likely to be changed (most of samples are obviously
not support vectors, their $\alpha_i$ will not be changed once it are 0). We loop over the samples
whose $\alpha$ is $0 < \alpha_i < C$ and choose those are against KKT conditions to update until all those
$0 < \alpha_i < C$ samples are valid for KKT. If all $0 < \alpha_i < C$ samples were not changed in one
outter loop, we will then have to loop over all samples. Then if there is any sample updated in this all
sample looping, we have to loop over those $0 < \alpha_i < C$ samples. So outter loop is switching between
looping over $0 < \alpha_i < C$ samples and looping over all samples until all samples are valid for KKT.
- inner loop: we already found the $\alpha_2$ which did not satisfied KKT condtions, we will try to find
a $\alpha_1$ that maximizes the $\|E_1 - E_2\|$. But sometims, the $\alpha_1$ and $\alpha_2$ pair we found
cannot be updated(for example, $\eta = 0$). So we will try to find new $\alpha_1$ by looping over all
$0 < \alpha_i < C$ samples. If still cannot find, then we loop over all samples. If still cannot find,
then we give up the $\alpha_2$ we found in outter loop.

### SMO in R ###

```{r eval=FALSE, tidy=FALSE}
ker <- function(X) {
  X %*% t(X)
}
```

```{r eval=FALSE, tidy=FALSE}
takeStep <- function(i1, i2, X, Y, C, E2, alpha, b, kernel){
  if (i1 == i2) {
		return(list(flag = 0, alpha = alpha, b = b))
	}
	eps <- 0.001
	alph1 <- alpha[i1] ##Lagrange multiplier for i1
	alph2 <- alpha[i2]
	y1 <- Y[i1]
	y2 <- Y[i2]
	u1 <- sum(kernel[i1, ]*Y*alpha) + b
	E1 <- u1 - y1 ##SVM output on point[i1] â€“ y1 (check in error cache)
	s <- y1*y2
	if (s == 1) { ##Compute L, H via equations (13) and (14)
		L <- max(0, alph1 + alph2 - C) 
		H <- min(C, alph2 + alph1)
	}else {
		L <- max(0, alph2 - alph1)
		H <- min(C, C + alph2 - alph1)
	} 
	if (L == H) {
		return(list(flag = 0, alpha = alpha, b = b))
	}
	k11 <- kernel[i1, i1]
	k12 <- kernel[i1, i2]
	k22 <- kernel[i2, i2]
	eta <- k11 + k22 - 2*k12
	if (eta > 0) {
		a2 <- alph2 + y2*(E1 - E2)/eta
		if (a2 < L) {
			a2 <- L
		}else if (a2 > H) {
			a2 <- H
		}
	}else {
		f1 <- y1*(E1 - b) - alph1*k11 - s*alph2*k12
		f2 <- y2*(E2 - b) - s*alph1*k12 - alph2*k22
		L1 <- alph1 + s*(alph2 - L)
		H1 <- alph1 + s*(alph2 - H)
		Lobj <- L1*f1 + L*f2 + 1/2*L1^2*k11 + 1/2*L^2*k22 + s*L*L1*k12 ##objective function at a2=L
		Hobj <- H1*f1 + H*f2 + 1/2*H1^2*k11 + 1/2*H^2*k22 + s*H*H1*k12 ##objective function at a2=H
		if (Lobj < Hobj - eps) {
			a2 <- L
		}else if(Lobj > Hobj + eps) {
			a2 <- H
		}else {
			a2 <- alph2
		}
	}
	if (abs(a2 - alph2) < eps*(a2 + alph2 + eps)) {
		return(list(flag = 0, alpha = alpha, b = b))
	}
	a1 <- alph1 + s*(alph2 - a2)
	##Update threshold to reflect change in Lagrange multipliers
	b1 <- b - (E1 + y1*(a1 - alph1)*k11 + y2*(a2 - alph2)*k12)  
	b2 <- b - (E2 + y1*(a1 - alph1)*k12 + y2*(a2 - alph2)*k22)  
	if(a1 < C & a1 > 0){
		b <- b1
	}else if(a2 < C & a2 > 0){
		b <- b2
	}else {
		b <- (b1 + b2)/2
	}
	##Update weight vector to reflect change in a1 & a2, if SVM is linear
	##Update error cache using new Lagrange multipliers
	alpha[i1] <- a1 ##Store a1 in the alpha array
	alpha[i2] <- a2 ##Store a2 in the alpha array
	return(list(flag = 1, alpha = alpha, b = b))
}
```

```{r eval=FALSE, tidy=FALSE}
examineExample <- function(i2, X, Y, alpha, b, C, kernel){
  y2 <- Y[i2]
	tol <- 0.001
	alph2 <- alpha[i2]
	u2 <- sum(kernel[i2, ]*Y*alpha) + b
	E2 <- u2 - y2 ##SVM output on point[i2] â€“ y2 (check in error cache)
	r2 <- E2*y2 ##r2 = y2*(u2 - y2) = y2*u2 - 1 = 0 is KKT condition, it will be used to check KTT violation
	if ((r2 < -tol && alph2 < C) | (r2 > tol && alph2 > 0)) {
		index <- which(alpha != C & alpha != 0)
		if (sum(alpha != C & alpha != 0) > 1) { 
			E1 <- kernel[index, ] %*% (Y*alpha) + rep(b, length(index))
			diff <- which.max(abs(E1 - rep(E2, length(E1))))
			i1 <- index[diff] ##result of second choice heuristic (section 2.2)
			result <- takeStep(i1, i2, X, Y, C, E2, alpha, b, kernel)
			if(result$flag) {
				return(list(flag = 1, alpha = result$alpha, b = result$b))
			}
		}
		##loop over all non-zero and non-C alpha, starting at a random point
		for(i in sample(index, length(index))) {
			i1 <- i
			result <- takeStep(i1, i2, X, Y, C, E2, alpha, b, kernel)
			if(result$flag) {
				return(list(flag = 1, alpha = result$alpha, b = result$b))
			}
		}
		##loop over all possible i1, starting at a random point
		for(j in sample(1:length(Y), length(Y))) {
			i1 <- j
			result <- takeStep(i1, i2, X, Y, C, E2, alpha, b, kernel)
			if(result$flag) {
				return(list(flag = 1, alpha = result$alpha, b = result$b))
			}
		}
	}
	return(list(flag = 0, alpha = alpha, b = b))
}
```

```{r eval=FALSE, tidy=FALSE}
SMO <- function(mean1, mean2, sd1, sd2, seed1, seed2) {
  library(MASS)
	set.seed(seed1)
	x1 <- mvrnorm(1000, mean1, sd1)
	set.seed(seed2)
	x2 <- mvrnorm(1000, mean2, sd2)
	X <- rbind(x1,x2)
	Y<- rep(c(1, -1), each = 1000)
	kernel <- ker(X)

	alpha <- rep(1,length(Y))
	b <- 0
	numChanged <- 0
	examineAll <- 1
  C <- 1

	while (numChanged > 0 | examineAll) {
		numChanged <- 0
		if (examineAll) {
			for(I in 1:length(Y)) { ##loop I over all training examples
				result <- examineExample(I, X, Y, alpha, b, C, kernel)
				numChanged <- numChanged + result$flag
				alpha <- result$alpha
				b <- result$b
			}
		}else {
			for(I in which(alpha != C & alpha != 0)) { ##loop I over examples where alpha is not 0 & not C
				result <- examineExample(I, X, Y, alpha, b, C, kernel)
				numChanged <- numChanged + result$flag
				alpha <- result$alpha
				b <- result$b
			}
		}
		if (examineAll == 1) {
			examineAll <- 0
		}else if (numChanged == 0) {
			examineAll <- 1
		}
	}
	w <- matrix(Y*alpha, nrow = 1) %*% X
	list(w,b)
}

SMO(mean1, mean2, sd1, sd2, seed1, seed2)
```