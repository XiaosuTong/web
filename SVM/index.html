<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Support Vector Machine</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    
    <link href="assets/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/custom/custom.css" rel="stylesheet">
    <!-- font-awesome -->
    <link href="assets/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    
    <!-- prism -->
    <link href="assets/prism/prism.css" rel="stylesheet">
    <link href="assets/prism/prism.r.css" rel="stylesheet">
    <script type='text/javascript' src='assets/prism/prism.js'></script>
    <script type='text/javascript' src='assets/prism/prism.r.js'></script>
    
    
    
    <script type="text/javascript" src="assets/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
   MathJax.Hub.Config({    
     extensions: ["tex2jax.js"],    
     "HTML-CSS": { scale: 100}    
   });
   </script>
    
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
    <![endif]-->
    
    <link href='http://fonts.googleapis.com/css?family=Lato' rel='stylesheet' type='text/css'>
    <!-- <link href='http://fonts.googleapis.com/css?family=Lustria' rel='stylesheet' type='text/css'> -->
    <link href='http://fonts.googleapis.com/css?family=Bitter' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Raleway:300' rel='stylesheet' type='text/css'>
    

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
                                   <!-- <link rel="shortcut icon" href="ico/favicon.png"> -->
  </head>

  <body>

    <div class="container-narrow">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
           
        </ul>
        
        <p class="myHeader">Support Vector Machine</p>
      </div>

      <hr>

<div class="container-fluid">
   <div class="row-fluid">
   
   <div class="col-md-3 well">
   <ul class = "nav nav-list" id="toc">
   <li class='nav-header unselectable' data-edit-href='01.intro.Rmd'>Defination</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#in-short'>In Short</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#motivation'>Motivation</a>
      </li>

   </ul>
   </div>

<div class="col-md-9 tab-content" id="main-content">

<div class='tab-pane active' id='in-short'>
<h3>In Short</h3>

<p>An SVM model is a representation of the examples as points in space, mapped so that the examples of
the separate categories are divided by a clear gap that is as wide as possible. New examples are 
then mapped into that same space and predicted to belong to a category based on which side of the 
gap they fall on.</p>

<p>It often happens that the sets to discriminate are not linearly separable in that space. For this 
reason, it was proposed that the original finite-dimensional space be mapped into a much 
higher-dimensional space, presumably making the separation easier in that space.</p>

<p>The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product
with a vector in that space is constant.</p>

</div>


<div class='tab-pane' id='motivation'>
<h3>Motivation</h3>

<p>In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of 
p numbers), and we want to know whether we can separate such points with a (p âˆ’ 1)-dimensional 
hyperplane. This is called a linear classifier. </p>

<p>There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane 
is the one that represents the largest separation, or margin, between the two classes. So we choose 
the hyperplane so that the distance from it to the nearest data point on each side is maximized. If 
such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it 
defines is known as a maximum margin classifier; or equivalently, the perceptron of optimal stability.</p>

<p>Any hyperplane can be written as the set of points \(\mathbf{x}\) satisfying</p>

<p>\[ 
\mathbf{w}\dot\mathbf{x} - b = 0
\]</p>

</div>

   
   <ul class="pager">
      <li><a href="#" id="previous">&larr; Previous</a></li> 
      <li><a href="#" id="next">Next &rarr;</a></li> 
   </ul>
</div>


</div>
</div>

<hr>

<div class="footer">
   <p>&copy; Xiaosu Tong, 2014</p>
</div>
</div> <!-- /container -->

<script src="assets/jquery/jquery.js"></script>
<script type='text/javascript' src='assets/custom/custom.js'></script>
<script src="assets/bootstrap/js/bootstrap.min.js"></script>
<script src="assets/custom/jquery.ba-hashchange.min.js"></script>
<script src="assets/custom/nav.js"></script>

</body>
</html>